{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning! Developing mode activated! \n"
     ]
    }
   ],
   "source": [
    "################################################ Loading libraries and arguments ################################################\n",
    "\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "\n",
    "test = True\n",
    "if test:\n",
    "    print('Warning! Developing mode activated! ')\n",
    "    input_fasta = '/home/jpereira/OEs/OE1.v2/Data/check_gc/decont.low_gc.fasta'\n",
    "    input_str_tsv = '/home/jpereira/OEs/OE1.v2/Data/short_tandem_repeats/short_tandem_repeats.tsv'\n",
    "    params_period_size = 0\n",
    "    params_total_tandem_coverage = 0\n",
    "    params_tandem_coverage = 0.85\n",
    "    output_passed_sequences = '/home/jpereira/OEs/OE1.v2/Data/short_tandem_repeats/str_filtered.fasta'\n",
    "    output_discarded_sequences = '/home/jpereira/OEs/OE1.v2/Data/short_tandem_repeats/str_filtered.fasta'\n",
    "    output_repeats_tsv = '/home/jpereira/OEs/OE1.v2/Data/short_tandem_repeats/filter_short_tandem_repeats.tsv'\n",
    "    output_repeats_fasta = '/home/jpereira/OEs/OE1.v2/Data/short_tandem_repeats/repeats.fasta'\n",
    "\n",
    "else:\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"\"\"\n",
    "        Discard sequences from a FASTA file if they have short tandem repeats (STR).\n",
    "        \"\"\")\n",
    "    parser.add_argument(\"--input-fasta\", help=\"Path to the input FASTA file.\")\n",
    "    parser.add_argument(\"--input-str-table\", help=\"Path to a TSV table with short tandem repeats present in the FASTA file.\")\n",
    "    \n",
    "    parser.add_argument(\"--param-period-size\", type=float, default=0, help=\"Minimum period size of an STR required to discard the sequence.\")\n",
    "    parser.add_argument(\"--param-tandem-coverage\", type=float, default=0.85, help=\"Coverage ratio of an STR found in the sequence (0-1) required to discard it.\")\n",
    "    parser.add_argument(\"--param-total-tandem-coverage\", type=float, default=0.85, help=\"Coverage ratio of all STR found in the sequence (0-1) required to discard the sequence.\")\n",
    "    \n",
    "    parser.add_argument(\"--output-passed-sequences\", help=\"Output FASTA file of sequences WITHOUT STR\", default=\"passed_sequences.fasta\")\n",
    "    parser.add_argument(\"--output-discarded-sequences\", help=\"Output FASTA file of sequences WITH STR\", default=\"discarded_sequences.fasta\")\n",
    "    parser.add_argument(\"--output-short-repeats\", help=\"Output FASTA file with the short repeats in each discarded sequence\")\n",
    "    parser.add_argument(\"--output-str-table\", help=\"A table with the STR data of sequences that didn't pass the filter\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    input_fasta = args.input_fasta\n",
    "    input_str_tsv = args.input_str_table\n",
    "        \n",
    "    params_tandem_coverage = args.param_tandem_coverage,\n",
    "    params_total_tandem_coverage = args.param_total_tandem_coverage\n",
    "    params_period_size = args.param_period_size\n",
    "    \n",
    "    output_passed_sequences = args.output_passed_sequences\n",
    "    output_discarded_sequences = args.output_discarded_sequences\n",
    "    output_repeats_fasta = args.output_short_repeats\n",
    "    output_repeats_tsv = args.output_str_table\n",
    "    \n",
    "    \n",
    "###### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ Helper functions used in the script ################################################\n",
    "\n",
    "def total_coverage_generic(cols: pd.DataFrame, start_coord_col_name: str, end_coord_col_name: str, seq_len_col_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the total coverage fraction of a sequence by merging overlapping intervals.\n",
    "\n",
    "    Parameters:\n",
    "        cols (pd.DataFrame): A DataFrame sorted by 'seqid', then by start and end coordinates.\n",
    "        start_coord_col_name (str): Name of the column with the start coordinate.\n",
    "        end_coord_col_name (str): Name of the column with the end coordinate.\n",
    "        seq_len_col_name (str): Name of the column with the sequence length.\n",
    "\n",
    "    Assumptions:\n",
    "        - The DataFrame contains only one 'seqid' or is grouped by 'seqid'.\n",
    "        - Coordinates are sorted beforehand.\n",
    "\n",
    "    Returns:\n",
    "        float: The fraction of the sequence covered by the merged intervals.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle empty DataFrame\n",
    "    if cols.empty:\n",
    "        return 0.0\n",
    "\n",
    "    sstart = cols[start_coord_col_name].values\n",
    "    send = cols[end_coord_col_name].values\n",
    "    total_len = cols[seq_len_col_name].iloc[0]\n",
    "    \n",
    "    total_cover = 0\n",
    "    current_start, current_end = sstart[0], send[0]\n",
    "\n",
    "    for i in range(1, len(sstart)):\n",
    "        if sstart[i] <= current_end:\n",
    "            current_end = max(current_end, send[i])\n",
    "        else:\n",
    "            total_cover += current_end - current_start\n",
    "            current_start, current_end = sstart[i], send[i]\n",
    "\n",
    "    total_cover += current_end - current_start\n",
    "    return total_cover / total_len if total_len > 0 else 0.0\n",
    "\n",
    "###### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'seqid'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_290385/3992635458.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Loading input STR table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mstr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_str_tsv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Add len sizes to str_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mstr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfasta_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'seqid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Evaluate each tandem line indivualy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# The dataframe is grouped by the column row in order to calculate the coverage of each tandem indiviauly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/OE1/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/OE1/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/OE1/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1306\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                         \u001b[0;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/OE1/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'seqid'"
     ]
    }
   ],
   "source": [
    "################################################ Loading inputs and filtering tandems ################################################\n",
    "\n",
    "# Loading input FASTA sequences \n",
    "infasta_dict = {'seqid' : [], 'qlen' : []}\n",
    "with open(input_fasta, 'r') as fasta:\n",
    "    for record in SeqIO.parse(fasta,'fasta'):\n",
    "        infasta_dict['seqid'].append(record.id)\n",
    "        infasta_dict['qlen'].append(len(record.seq))\n",
    "infasta_df = pd.DataFrame(infasta_dict)\n",
    "\n",
    "# Loading input STR table\n",
    "str_df = pd.read_csv(input_str_tsv, sep='\\t')\n",
    "\n",
    "# Add len sizes to str_df \n",
    "str_df = pd.merge(str_df, infasta_df, how='left', on='seqid')\n",
    "\n",
    "# Evaluate each tandem line indivualy\n",
    "# The dataframe is grouped by the column row in order to calculate the coverage of each tandem indiviauly.\n",
    "# If were grouped by 'seqid' all tandem coverge would be summed\n",
    "str_df['row'] = np.linspace(1, str_df.shape[0], num=str_df.shape[0])\n",
    "tandem_coverage = str_df.sort_values(by=['seqid','rstart', 'rend']).groupby('row').apply( lambda x: total_coverage_generic(x, 'rstart', 'rend', 'qlen')).reset_index(name='tandem_coverage_fraction')\n",
    "str_df = pd.merge(str_df, tandem_coverage, on='row', how='left')\n",
    "\n",
    "###### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################### Saving filtered sequences, repeats and filtered str-table ######################################\n",
    "\n",
    "\n",
    "#Extract relevant STR and save them as a TSV \n",
    "filtered_str_df = str_df[str_df['tandem_coverage_fraction'] > params_tandem_coverage]\n",
    "filtered_str_df.to_csv(output_repeats_tsv, sep='\\t', index=False)\n",
    "\n",
    "#Write relevant STR sequneces as individual FASTA entries\n",
    "repeats_df = filtered_str_df[['seqid', 'repeat_sequence']]\n",
    "\n",
    "ids_counts = {}\n",
    "with open(output_repeats_fasta, 'w') as fasta:\n",
    "    for _, row in repeats_df.iterrows():\n",
    "        seqid = row['seqid']\n",
    "        sequence = row['repeat_sequence']\n",
    "\n",
    "        # Ensure unique identifiers\n",
    "        ids_counts[seqid] = ids_counts.get(seqid, 0) + 1\n",
    "        fasta_id = f\"{seqid}_{ids_counts[seqid]}\"\n",
    "\n",
    "        fasta.write(f\">{fasta_id}\\n{sequence}\\n\")\n",
    "\n",
    "# Create a set for fast lookup\n",
    "tandem_ids = set(repeats_df['seqid'])\n",
    "\n",
    "# Separate sequences from the input FASTA based on whether they have tandem repeats\n",
    "with open(input_fasta, 'r') as in_fasta, \\\n",
    "     open(output_discarded_sequences, 'w') as discard_fasta, \\\n",
    "     open(output_passed_sequences, 'w') as filter_fasta:\n",
    "\n",
    "    for record in SeqIO.parse(in_fasta, 'fasta'):\n",
    "        if record.id in tandem_ids:\n",
    "            SeqIO.write(record, discard_fasta, 'fasta')\n",
    "        else:\n",
    "            SeqIO.write(record, filter_fasta, 'fasta')\n",
    "\n",
    "###### - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  - - - - - - - - - - - - - - -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OE1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
